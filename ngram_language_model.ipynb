{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modelling Lab (week 2)\n",
    "This notebook provides the \"starter\" code in the week 2 lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "We need to get the names of files in the training directory and split them into training and testing 50:50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 522 files in the training directory: sentence-completion/Holmes_Training_Data\n"
     ]
    }
   ],
   "source": [
    "import os,random,math\n",
    "TRAINING_DIR=\"sentence-completion/Holmes_Training_Data\"  #this needs to be the parent directory for the training corpus\n",
    "\n",
    "def get_training_testing(training_dir=TRAINING_DIR,split=0.5):\n",
    "    \n",
    "    filenames=os.listdir(training_dir)\n",
    "    n=len(filenames)\n",
    "    print(\"There are {} files in the training directory: {}\".format(n,training_dir))\n",
    "    random.seed(53)  #if you want the same random split every time\n",
    "    random.shuffle(filenames)\n",
    "    index=int(n*split)\n",
    "    return(filenames[:index],filenames[index:])\n",
    "    \n",
    "trainingfiles,heldoutfiles=get_training_testing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainingfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1  Building a unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize as tokenize\n",
    "import operator\n",
    "import random\n",
    "\n",
    "class language_model():\n",
    "    \n",
    "    def __init__(self,trainingdir=TRAINING_DIR,files=[]):\n",
    "        #store the names of the files containing training data and run the training method\n",
    "        self.training_dir=trainingdir\n",
    "        self.files=files\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "    def train(self):\n",
    "        #initialise an empty dictionary which will be the unigram model {w:P(w)} when training is complete\n",
    "        self.unigram={}\n",
    "        #process all of the training data, accumulating counts of events\n",
    "        self._processfiles()\n",
    "        #convert the accumulated counts to probabilities\n",
    "        self._convert_to_probs()\n",
    "        \n",
    "    def _processline(self,line):\n",
    "        #process each line of a file\n",
    "        #each line is tokenized and has a special start and end token added\n",
    "        #counts of tokens are added to the self.unigram count model\n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        for token in tokens:\n",
    "            self.unigram[token]=self.unigram.get(token,0)+1\n",
    "    \n",
    "    \n",
    "    def _processfiles(self):\n",
    "        #process each file in turn\n",
    "        for afile in self.files:\n",
    "            print(\"Processing {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            self._processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n",
    "      \n",
    "            \n",
    "    def _convert_to_probs(self):\n",
    "        #self.unigram initially counts counts for each token {token:freq(token)}\n",
    "        #sum all of the frequencies and divide each frequency by that sum to get probabilities\n",
    "        \n",
    "        self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
    "    \n",
    "    \n",
    "    def get_prob(self,token,method=\"unigram\"):\n",
    "        #simple look up method\n",
    "        if method==\"unigram\":\n",
    "            return self.unigram.get(token,0)\n",
    "        else:\n",
    "            print(\"Not implemented: {}\".format(method))\n",
    "            return 0  \n",
    "    \n",
    "    def get_common_words(self,k=10,stopword=\"__END\"):\n",
    "        #keep returning one of the highly probable words until a stopword is encountered or the max length is exceeded\n",
    "        \n",
    "        blacklist = [\"__START\"]\n",
    "        sorted_unigram = sorted(self.unigram.items(), key=lambda item: item[1], reverse=True)\n",
    "        filtered = [w for (w,p) in sorted_unigram if w not in blacklist]\n",
    "        \n",
    "        chosen_words = []\n",
    "        for i in range(k):\n",
    "            rand_word = random.choice(filtered[:k])\n",
    "            if rand_word == stopword:\n",
    "                break\n",
    "            chosen_words.append(rand_word)\n",
    "                \n",
    "        return \" \".join(chosen_words[:-1])\n",
    "        \n",
    "        \n",
    "    def get_sample_from_dist(self,k=10,stopword='__END'):\n",
    "        #return one of the highly probable words\n",
    "        \n",
    "        blacklist = [\"__START\"]\n",
    "        sorted_unigram = sorted(self.unigram.items(), key=lambda item: item[1], reverse=True)\n",
    "        filtered = [(w,p) for (w,p) in sorted_unigram if w not in blacklist]\n",
    "        values,dist = zip(*filtered[:k])\n",
    "        \n",
    "        chosen_words = []\n",
    "        for i in range(k):\n",
    "            rand_word = random.choices(values,dist)[0]\n",
    "            if rand_word == stopword:\n",
    "                break\n",
    "            chosen_words.append(rand_word)\n",
    "                \n",
    "        return \" \".join(chosen_words[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DYNMT10.TXT\n",
      "Processing 09TOM10.TXT\n",
      "Processing PRSIT10.TXT\n",
      "Processing NWIND10.TXT\n",
      "Processing BDAPH10.TXT\n"
     ]
    }
   ],
   "source": [
    "MAX_FILES=5\n",
    "mylm=language_model(files=trainingfiles[:MAX_FILES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03527550407241448"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.get_prob('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"of . . , '' I the , .\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.get_common_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you look up some probabilities of words in your model.  Pick some words which you would expect to have high probabilities and some words which you would expect to have low probabilities.\n",
    "\n",
    "As an extension, see how these change if you use a bigger portion of the training data to train your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you . at . , ,'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.get_sample_from_dist(k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize as tokenize\n",
    "import operator\n",
    "import random\n",
    "import math\n",
    "\n",
    "class language_model():\n",
    "    \n",
    "    def __init__(self,trainingdir=TRAINING_DIR,files=[]):\n",
    "        #store the names of the files containing training data and run the training method\n",
    "        self.training_dir=trainingdir\n",
    "        self.files=files\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "    def train(self):\n",
    "        #initialise an empty dictionary which will be the unigram model {w:P(w)} when training is complete\n",
    "        self.unigram={}\n",
    "        #initialise an empty dictionary which will be the bigram model\n",
    "        self.bigram={}\n",
    "        #process all of the training data, accumulating counts of events\n",
    "        self._processfiles()\n",
    "        #words in the corpus\n",
    "        self.word_count = sum(self.unigram.values())\n",
    "        #convert the accumulated counts to probabilities\n",
    "        self._convert_to_probs()\n",
    "        \n",
    "    def _processline(self,line):\n",
    "        #process each line of a file\n",
    "        #each line is tokenized and has a special start and end token added\n",
    "        #counts of tokens are added to the self.unigram count model\n",
    "        \n",
    "        #unigram\n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        for token in tokens:\n",
    "            self.unigram[token]=self.unigram.get(token,0)+1\n",
    "        \n",
    "        #bigram\n",
    "        tokens_bg = [\"__START\"]+tokenize(line)\n",
    "        for index in range(len(tokens_bg)-1):\n",
    "            if self.bigram.get(tokens_bg[index],0) == 0:\n",
    "                self.bigram[tokens_bg[index]] = {}\n",
    "            \n",
    "            self.bigram[tokens_bg[index]][tokens_bg[index+1]] = self.bigram[tokens_bg[index]].get(tokens_bg[index+1],0)+1            \n",
    "    \n",
    "    def _processfiles(self):\n",
    "        #process each file in turn\n",
    "        for afile in self.files:\n",
    "            print(\"Processing {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            self._processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n",
    "      \n",
    "            \n",
    "    def _convert_to_probs(self):\n",
    "        #self.unigram initially counts counts for each token {token:freq(token)}\n",
    "        #sum all of the frequencies and divide each frequency by that sum to get probabilities\n",
    "        \n",
    "        #unigram\n",
    "        self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
    "        \n",
    "        #bigram\n",
    "        for outer_key,inner_dict in self.bigram.items():\n",
    "            inner_dict_probs = {k:v/sum(inner_dict.values()) for (k,v) in inner_dict.items()}\n",
    "            self.bigram[outer_key] = inner_dict_probs\n",
    "            \n",
    "                   \n",
    "    def get_prob(self,token,method=\"unigram\"):\n",
    "        #simple look up method\n",
    "        if method==\"unigram\":\n",
    "            return self.unigram.get(token,0)\n",
    "        else:\n",
    "            print(\"Not implemented: {}\".format(method))\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def get_common_words(self,k=10,stopword='.'):\n",
    "        \n",
    "        sorted_unigram = sorted(self.unigram.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "        chosen_words = []\n",
    "        for i in range(k):\n",
    "            rand_word = random.choice(sorted_unigram[:k])\n",
    "            chosen_words.append(rand_word)\n",
    "            if rand_word[0] == stopword:\n",
    "                break\n",
    "                \n",
    "        return chosen_words\n",
    "        \n",
    "    def get_sample_from_dist(self,k=10,stopword='__END', method=\"unigram\"):\n",
    "        #return one of the highly probable words\n",
    "        \n",
    "        current_word = \"__START\"\n",
    "        blacklist = [\"__START\"]\n",
    "\n",
    "        if method == \"unigram\":\n",
    "            sorted_unigram = sorted(self.unigram.items(), key=lambda item: item[1], reverse=True)\n",
    "            filtered = [(w,p) for (w,p) in sorted_unigram if w not in blacklist]\n",
    "            values,dist = zip(*filtered[:k])\n",
    "        \n",
    "            chosen_words = []\n",
    "            for i in range(k):\n",
    "                current_word = random.choices(values,dist)[0]\n",
    "                if current_word == stopword:\n",
    "                    break\n",
    "                chosen_words.append(current_word)\n",
    "\n",
    "            return \" \".join(chosen_words[:-1])\n",
    "        \n",
    "        \n",
    "        elif method == \"bigram\":       \n",
    "            sorted_bigram = sorted(self.bigram[current_word].items(), key=lambda item: item[1], reverse=True)\n",
    "            filtered = [(w,p) for (w,p) in sorted_bigram if w not in blacklist]\n",
    "            values,dist = zip(*filtered[:k])\n",
    "            \n",
    "            chosen_words = []\n",
    "            for i in range(k):\n",
    "                sorted_bigram = sorted(self.bigram[current_word].items(), key=lambda item: item[1], reverse=True)\n",
    "                filtered = [(w,p) for (w,p) in sorted_bigram if w not in blacklist]\n",
    "                values,dist = zip(*filtered[:k])\n",
    "                current_word = random.choices(values,dist)[0]\n",
    "                if current_word == stopword:\n",
    "                    break\n",
    "                chosen_words.append(current_word)\n",
    "            \n",
    "            return \" \".join(chosen_words[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DYNMT10.TXT\n",
      "Processing 09TOM10.TXT\n",
      "Processing PRSIT10.TXT\n",
      "Processing NWIND10.TXT\n",
      "Processing BDAPH10.TXT\n"
     ]
    }
   ],
   "source": [
    "lang_ml = language_model(files=trainingfiles[:MAX_FILES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"was a light in the house , for me in the next meet ? '' said nothing to a hand ? '' said Berry . Then suddenly conquered a girl 's not even , I should n't it ,\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_ml.get_sample_from_dist(method=\"bigram\", k=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize as tokenize\n",
    "import operator\n",
    "import random\n",
    "import math\n",
    "\n",
    "class language_model():\n",
    "    \n",
    "    def __init__(self,trainingdir=TRAINING_DIR,files=[]):\n",
    "        #store the names of the files containing training data and run the training method\n",
    "        self.training_dir=trainingdir\n",
    "        self.files=files\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "    def train(self):\n",
    "        #initialise an empty dictionary which will be the unigram model {w:P(w)} when training is complete\n",
    "        self.unigram={}\n",
    "        #initialise an empty dictionary which will be the bigram model\n",
    "        self.bigram={}\n",
    "        #process all of the training data, accumulating counts of events\n",
    "        self._processfiles()\n",
    "        #words in the corpus\n",
    "        self.word_count = sum(self.unigram.values())\n",
    "        #convert the accumulated counts to probabilities\n",
    "        self._convert_to_probs()\n",
    "        \n",
    "    def _processline(self,line):\n",
    "        #process each line of a file\n",
    "        #each line is tokenized and has a special start and end token added\n",
    "        #counts of tokens are added to the self.unigram count model\n",
    "        \n",
    "        #unigram\n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        for token in tokens:\n",
    "            self.unigram[token]=self.unigram.get(token,0)+1\n",
    "        \n",
    "        #bigram\n",
    "        tokens_bg = [\"__START\"]+tokenize(line)\n",
    "        for index in range(len(tokens_bg)-1):\n",
    "            if self.bigram.get(tokens_bg[index],0) == 0:\n",
    "                self.bigram[tokens_bg[index]] = {}\n",
    "            \n",
    "            self.bigram[tokens_bg[index]][tokens_bg[index+1]] = self.bigram[tokens_bg[index]].get(tokens_bg[index+1],0)+1            \n",
    "    \n",
    "    def _processfiles(self):\n",
    "        #process each file in turn\n",
    "        for afile in self.files:\n",
    "            print(\"Processing {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            self._processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n",
    "      \n",
    "            \n",
    "    def _convert_to_probs(self):\n",
    "        #self.unigram initially counts counts for each token {token:freq(token)}\n",
    "        #sum all of the frequencies and divide each frequency by that sum to get probabilities\n",
    "        \n",
    "        #unigram\n",
    "        self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
    "        \n",
    "        #bigram\n",
    "        for outer_key,inner_dict in self.bigram.items():\n",
    "            inner_dict_probs = {k:v/sum(inner_dict.values()) for (k,v) in inner_dict.items()}\n",
    "            self.bigram[outer_key] = inner_dict_probs\n",
    "            \n",
    "                   \n",
    "    def get_prob(self,token,context=\"\",method=\"unigram\"):\n",
    "        #simple look up method\n",
    "        if method==\"unigram\":\n",
    "            return self.unigram.get(token,0)\n",
    "        elif method==\"bigram\":\n",
    "            return self.bigram.get(context[-1],{}).get(token,0)\n",
    "    \n",
    "    \n",
    "    def get_common_words(self,k=10,stopword='.'):\n",
    "        \n",
    "        sorted_unigram = sorted(self.unigram.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "        chosen_words = []\n",
    "        for i in range(k):\n",
    "            rand_word = random.choice(sorted_unigram[:k])\n",
    "            chosen_words.append(rand_word)\n",
    "            if rand_word[0] == stopword:\n",
    "                break\n",
    "                \n",
    "        return chosen_words\n",
    "        \n",
    "    def get_sample_from_dist(self,k=10,stopword='__END', method=\"unigram\"):\n",
    "        #return one of the highly probable words\n",
    "        \n",
    "        current_word = \"__START\"\n",
    "        blacklist = [\"__START\"]\n",
    "\n",
    "        if method == \"unigram\":\n",
    "            sorted_unigram = sorted(self.unigram.items(), key=lambda item: item[1], reverse=True)\n",
    "            filtered = [(w,p) for (w,p) in sorted_unigram if w not in blacklist]\n",
    "            values,dist = zip(*filtered[:k])\n",
    "        \n",
    "            chosen_words = []\n",
    "            for i in range(k):\n",
    "                current_word = random.choices(values,dist)[0]\n",
    "                if current_word == stopword:\n",
    "                    break\n",
    "                chosen_words.append(current_word)\n",
    "\n",
    "            return \" \".join(chosen_words[:-1])\n",
    "        \n",
    "        \n",
    "        elif method == \"bigram\":       \n",
    "            sorted_bigram = sorted(self.bigram[current_word].items(), key=lambda item: item[1], reverse=True)\n",
    "            filtered = [(w,p) for (w,p) in sorted_bigram if w not in blacklist]\n",
    "            values,dist = zip(*filtered[:k])\n",
    "            \n",
    "            chosen_words = []\n",
    "            for i in range(k):\n",
    "                sorted_bigram = sorted(self.bigram[current_word].items(), key=lambda item: item[1], reverse=True)\n",
    "                filtered = [(w,p) for (w,p) in sorted_bigram if w not in blacklist]\n",
    "                values,dist = zip(*filtered[:k])\n",
    "                current_word = random.choices(values,dist)[0]\n",
    "                if current_word == stopword:\n",
    "                    break\n",
    "                chosen_words.append(current_word)\n",
    "            \n",
    "            return \" \".join(chosen_words[:-1])\n",
    "        \n",
    "    def compute_prob_line(self,line,method=\"unigram\"):\n",
    "        #this will add _start to the beginning of a line of text\n",
    "        #compute the probability of the line according to the desired model\n",
    "        #and returns probability together with number of tokens\n",
    "    \n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        acc=0\n",
    "        for i,token in enumerate(tokens[1:]):\n",
    "            acc+=math.log(self.get_prob(token,tokens[:i+1],method))\n",
    "        return acc,len(tokens[1:])\n",
    "       \n",
    "    \n",
    "    def compute_probability(self,filenames=[],method=\"unigram\"):\n",
    "        #computes the probability (and length) of a corpus contained in filenames\n",
    "        if filenames==[]:\n",
    "            filenames=self.files\n",
    "        \n",
    "        total_p=0\n",
    "        total_N=0\n",
    "        for i,afile in enumerate(filenames):\n",
    "            print(\"Processing file {}:{}\".format(i,afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            p,N=self.compute_prob_line(line,method=method)\n",
    "                            total_p+=p\n",
    "                            total_N+=N\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing file {}: ignoring rest of file\".format(afile))\n",
    "        return total_p,total_N\n",
    "    \n",
    "    def compute_perplexity(self,filenames=[],method=\"unigram\"):\n",
    "        \n",
    "        #compute the probability and length of the corpus\n",
    "        #calculate perplexity\n",
    "        #lower perplexity means that the model better explains the data\n",
    "        \n",
    "        p,N=self.compute_probability(filenames=filenames,method=method)\n",
    "        #print(p,N)\n",
    "        pp=math.exp(-p/N)\n",
    "        return pp   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DYNMT10.TXT\n",
      "Processing 09TOM10.TXT\n",
      "Processing PRSIT10.TXT\n",
      "Processing NWIND10.TXT\n",
      "Processing BDAPH10.TXT\n",
      "Processing file 0:DYNMT10.TXT\n",
      "Processing file 1:09TOM10.TXT\n",
      "Processing file 2:PRSIT10.TXT\n",
      "Processing file 3:NWIND10.TXT\n",
      "Processing file 4:BDAPH10.TXT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "487.89637689524403"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ml = language_model(files=trainingfiles[:MAX_FILES])\n",
    "my_ml.compute_perplexity(trainingfiles[:MAX_FILES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 0:GGIRL10.TXT\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmy_ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheldoutfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mMAX_FILES\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mlanguage_model.compute_perplexity\u001b[1;34m(self, filenames, method)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_perplexity\u001b[39m(\u001b[38;5;28mself\u001b[39m,filenames\u001b[38;5;241m=\u001b[39m[],method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munigram\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    165\u001b[0m     \n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m#compute the probability and length of the corpus\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m#calculate perplexity\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m#lower perplexity means that the model better explains the data\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m     p,N\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_probability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilenames\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m#print(p,N)\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     pp\u001b[38;5;241m=\u001b[39mmath\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mp\u001b[38;5;241m/\u001b[39mN)\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mlanguage_model.compute_probability\u001b[1;34m(self, filenames, method)\u001b[0m\n\u001b[0;32m    155\u001b[0m line\u001b[38;5;241m=\u001b[39mline\u001b[38;5;241m.\u001b[39mrstrip()\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 157\u001b[0m     p,N\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_prob_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     total_p\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mp\n\u001b[0;32m    159\u001b[0m     total_N\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mN\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mlanguage_model.compute_prob_line\u001b[1;34m(self, line, method)\u001b[0m\n\u001b[0;32m    137\u001b[0m acc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens[\u001b[38;5;241m1\u001b[39m:]):\n\u001b[1;32m--> 139\u001b[0m     acc\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m acc,\u001b[38;5;28mlen\u001b[39m(tokens[\u001b[38;5;241m1\u001b[39m:])\n",
      "\u001b[1;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "my_ml.compute_perplexity(heldoutfiles[:MAX_FILES])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tackling Unknown Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize as tokenize\n",
    "import operator\n",
    "import random\n",
    "import math\n",
    "\n",
    "class language_model():\n",
    "    \n",
    "    def __init__(self,trainingdir=TRAINING_DIR,files=[]):\n",
    "        #store the names of the files containing training data and run the training method\n",
    "        self.training_dir=trainingdir\n",
    "        self.files=files\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "    def train(self):\n",
    "        #initialise an empty dictionary which will be the unigram model {w:P(w)} when training is complete\n",
    "        self.unigram={}\n",
    "        #initialise an empty dictionary which will be the bigram model\n",
    "        self.bigram={}\n",
    "        #process all of the training data, accumulating counts of events\n",
    "        self._processfiles()\n",
    "        #words in the corpus\n",
    "        self.word_count = sum(self.unigram.values())\n",
    "        #consider rare words as unknown\n",
    "        self.create_unknown()\n",
    "        #convert the accumulated counts to probabilities\n",
    "        self._convert_to_probs()\n",
    "        \n",
    "    def _processline(self,line):\n",
    "        #process each line of a file\n",
    "        #each line is tokenized and has a special start and end token added\n",
    "        #counts of tokens are added to the self.unigram count model\n",
    "        \n",
    "        #unigram\n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        for token in tokens:\n",
    "            self.unigram[token]=self.unigram.get(token,0)+1\n",
    "        \n",
    "        #bigram\n",
    "        tokens_bg = [\"__START\"]+tokenize(line)\n",
    "        for index in range(len(tokens_bg)-1):\n",
    "            if self.bigram.get(tokens_bg[index],0) == 0:\n",
    "                self.bigram[tokens_bg[index]] = {}\n",
    "            \n",
    "            self.bigram[tokens_bg[index]][tokens_bg[index+1]] = self.bigram[tokens_bg[index]].get(tokens_bg[index+1],0)+1            \n",
    "    \n",
    "    def _processfiles(self):\n",
    "        #process each file in turn\n",
    "        for afile in self.files:\n",
    "            print(\"Processing {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            self._processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n",
    "    \n",
    "    \n",
    "    def create_unknown(self):\n",
    "        \n",
    "        unk_keys = []\n",
    "        self.unigram[\"_UNK\"] = 0\n",
    "        for w,f in self.unigram.items():\n",
    "            if f <= 2:\n",
    "                self.unigram[\"_UNK\"] = self.unigram.get(\"_UNK\", 0) + f\n",
    "                unk_keys.append(w)\n",
    "        \n",
    "        for i in unk_keys:\n",
    "            del self.unigram[i]\n",
    "        \n",
    "        unk_outer_keys = []\n",
    "        self.bigram[\"_UNK\"] = {}\n",
    "        for outer,inner in self.bigram.items():\n",
    "            unk_inner_keys = []\n",
    "            inner[\"_UNK\"] = 0\n",
    "            for w,f in inner.items():\n",
    "                if f <= 2:\n",
    "                    inner[\"_UNK\"] = inner.get(\"_UNK\", 0) + f\n",
    "                    unk_inner_keys.append(w)\n",
    "            for i in unk_inner_keys:\n",
    "                del self.bigram[outer][i]\n",
    "            \n",
    "            \n",
    "            if outer not in self.unigram.keys():\n",
    "                self.bigram[\"_UNK\"] |= self.bigram[outer]\n",
    "                unk_outer_keys.append(outer)\n",
    "        for i in unk_outer_keys:\n",
    "            del self.bigram[i] \n",
    "    \n",
    "    def _convert_to_probs(self):\n",
    "        #self.unigram initially counts counts for each token {token:freq(token)}\n",
    "        #sum all of the frequencies and divide each frequency by that sum to get probabilities\n",
    "        \n",
    "        #unigram\n",
    "        self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
    "        \n",
    "        #bigram\n",
    "        for outer_key,inner_dict in self.bigram.items():\n",
    "            inner_dict_probs = {k:v/sum(inner_dict.values()) for (k,v) in inner_dict.items()}\n",
    "            self.bigram[outer_key] = inner_dict_probs\n",
    "            \n",
    "                   \n",
    "    def get_prob(self,token,context=\"\",method=\"unigram\"):\n",
    "        #simple look up method\n",
    "        if method==\"unigram\":\n",
    "            return self.unigram.get(token,self.unigram[\"_UNK\"])\n",
    "        elif method==\"bigram\":\n",
    "            return self.bigram.get(context[-1],self.bigram[\"_UNK\"]).get(token,self.bigram[context[-1]][\"_UNK\"])\n",
    "    \n",
    "    \n",
    "    def get_common_words(self,k=10,stopword='.'):\n",
    "        \n",
    "        sorted_unigram = sorted(self.unigram.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "        chosen_words = []\n",
    "        for i in range(k):\n",
    "            rand_word = random.choice(sorted_unigram[:k])\n",
    "            chosen_words.append(rand_word)\n",
    "            if rand_word[0] == stopword:\n",
    "                break\n",
    "                \n",
    "        return chosen_words\n",
    "        \n",
    "    def get_sample_from_dist(self,k=10,stopword='__END', method=\"unigram\"):\n",
    "        #return one of the highly probable words\n",
    "        \n",
    "        current_word = \"__START\"\n",
    "        blacklist = [\"__START\"]\n",
    "\n",
    "        if method == \"unigram\":\n",
    "            sorted_unigram = sorted(self.unigram.items(), key=lambda item: item[1], reverse=True)\n",
    "            filtered = [(w,p) for (w,p) in sorted_unigram if w not in blacklist]\n",
    "            values,dist = zip(*filtered[:k])\n",
    "        \n",
    "            chosen_words = []\n",
    "            for i in range(k):\n",
    "                current_word = random.choices(values,dist)[0]\n",
    "                if current_word == stopword:\n",
    "                    break\n",
    "                chosen_words.append(current_word)\n",
    "\n",
    "            return \" \".join(chosen_words[:-1])\n",
    "        \n",
    "        \n",
    "        elif method == \"bigram\":       \n",
    "            sorted_bigram = sorted(self.bigram[current_word].items(), key=lambda item: item[1], reverse=True)\n",
    "            filtered = [(w,p) for (w,p) in sorted_bigram if w not in blacklist]\n",
    "            values,dist = zip(*filtered[:k])\n",
    "            \n",
    "            chosen_words = []\n",
    "            for i in range(k):\n",
    "                sorted_bigram = sorted(self.bigram[current_word].items(), key=lambda item: item[1], reverse=True)\n",
    "                filtered = [(w,p) for (w,p) in sorted_bigram if w not in blacklist]\n",
    "                values,dist = zip(*filtered[:k])\n",
    "                current_word = random.choices(values,dist)[0]\n",
    "                if current_word == stopword:\n",
    "                    break\n",
    "                chosen_words.append(current_word)\n",
    "            \n",
    "            return \" \".join(chosen_words[:-1])\n",
    "        \n",
    "    def compute_prob_line(self,line,method=\"unigram\"):\n",
    "        #this will add _start to the beginning of a line of text\n",
    "        #compute the probability of the line according to the desired model\n",
    "        #and returns probability together with number of tokens\n",
    "    \n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        acc=0\n",
    "        for i,token in enumerate(tokens[1:]):\n",
    "            acc+=math.log(self.get_prob(token,tokens[:i+1],method))\n",
    "        return acc,len(tokens[1:])\n",
    "       \n",
    "    \n",
    "    def compute_probability(self,filenames=[],method=\"unigram\"):\n",
    "        #computes the probability (and length) of a corpus contained in filenames\n",
    "        if filenames==[]:\n",
    "            filenames=self.files\n",
    "        \n",
    "        total_p=0\n",
    "        total_N=0\n",
    "        for i,afile in enumerate(filenames):\n",
    "            print(\"Processing file {}:{}\".format(i,afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            p,N=self.compute_prob_line(line,method=method)\n",
    "                            total_p+=p\n",
    "                            total_N+=N\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing file {}: ignoring rest of file\".format(afile))\n",
    "        return total_p,total_N\n",
    "    \n",
    "    def compute_perplexity(self,filenames=[],method=\"unigram\"):\n",
    "        #compute the probability and length of the corpus\n",
    "        #calculate perplexity\n",
    "        #lower perplexity means that the model better explains the data\n",
    "        \n",
    "        p,N=self.compute_probability(filenames=filenames,method=method)\n",
    "        #print(p,N)\n",
    "        pp=math.exp(-p/N)\n",
    "        return pp   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DYNMT10.TXT\n",
      "Processing 09TOM10.TXT\n",
      "Processing PRSIT10.TXT\n",
      "Processing NWIND10.TXT\n",
      "Processing BDAPH10.TXT\n"
     ]
    }
   ],
   "source": [
    "my_new_ml = language_model(files=trainingfiles[:MAX_FILES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 0:GGIRL10.TXT\n",
      "Processing file 1:SBRUN10.TXT\n",
      "Processing file 2:TARZ510.TXT\n",
      "Processing file 3:ASPRN10.TXT\n",
      "Processing file 4:TBTAS10.TXT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "314.40734307861635"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_new_ml.compute_perplexity(heldoutfiles[:MAX_FILES])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize as tokenize\n",
    "import operator\n",
    "import random\n",
    "import math\n",
    "\n",
    "class language_model():\n",
    "    \n",
    "    def __init__(self,trainingdir=TRAINING_DIR,files=[]):\n",
    "        #store the names of the files containing training data and run the training method\n",
    "        self.training_dir=trainingdir\n",
    "        self.files=files\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "    def train(self):\n",
    "        #initialise an empty dictionary which will be the unigram model {w:P(w)} when training is complete\n",
    "        self.unigram={}\n",
    "        #initialise an empty dictionary which will be the bigram model\n",
    "        self.bigram={}\n",
    "        #process all of the training data, accumulating counts of events\n",
    "        self._processfiles()\n",
    "        #words in the corpus\n",
    "        self.word_count = sum(self.unigram.values())\n",
    "        #consider rare words as unknown\n",
    "        self.create_unknown()\n",
    "        #apply discount to the bigrams\n",
    "        self.apply_discount()\n",
    "        #convert the accumulated counts to probabilities\n",
    "        self._convert_to_probs()\n",
    "        \n",
    "    def _processline(self,line):\n",
    "        #process each line of a file\n",
    "        #each line is tokenized and has a special start and end token added\n",
    "        #counts of tokens are added to the self.unigram count model\n",
    "        \n",
    "        #unigram\n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        for token in tokens:\n",
    "            self.unigram[token]=self.unigram.get(token,0)+1\n",
    "        \n",
    "        #bigram\n",
    "        tokens_bg = [\"__START\"]+tokenize(line)\n",
    "        for index in range(len(tokens_bg)-1):\n",
    "            if self.bigram.get(tokens_bg[index],0) == 0:\n",
    "                self.bigram[tokens_bg[index]] = {}\n",
    "            \n",
    "            self.bigram[tokens_bg[index]][tokens_bg[index+1]] = self.bigram[tokens_bg[index]].get(tokens_bg[index+1],0)+1            \n",
    "    \n",
    "    def _processfiles(self):\n",
    "        #process each file in turn\n",
    "        for afile in self.files:\n",
    "            print(\"Processing {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            self._processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n",
    "    \n",
    "    \n",
    "    def create_unknown(self):\n",
    "        \n",
    "        unk_keys = []\n",
    "        self.unigram[\"_UNK\"] = 0\n",
    "        for w,f in self.unigram.items():\n",
    "            if f <= 2:\n",
    "                self.unigram[\"_UNK\"] = self.unigram.get(\"_UNK\", 0) + f\n",
    "                unk_keys.append(w)\n",
    "        \n",
    "        for i in unk_keys:\n",
    "            del self.unigram[i]\n",
    "        \n",
    "        unk_outer_keys = []\n",
    "        self.bigram[\"_UNK\"] = {}\n",
    "        for outer,inner in self.bigram.items():\n",
    "            unk_inner_keys = []\n",
    "            self.bigram[outer][\"_UNK\"] = 0\n",
    "            for w,f in inner.items():\n",
    "                if f <= 2:\n",
    "                    self.bigram[outer][\"_UNK\"] = self.bigram[outer].get(\"_UNK\", 0) + f\n",
    "                    unk_inner_keys.append(w)\n",
    "            for i in unk_inner_keys:\n",
    "                del self.bigram[outer][i]\n",
    "            \n",
    "            \n",
    "            if outer not in self.unigram.keys():\n",
    "                self.bigram[\"_UNK\"] |= self.bigram[outer]\n",
    "                unk_outer_keys.append(outer)\n",
    "        for i in unk_outer_keys:\n",
    "            del self.bigram[i] \n",
    "    \n",
    "    \n",
    "    def _convert_to_probs(self):\n",
    "        #self.unigram initially counts counts for each token {token:freq(token)}\n",
    "        #sum all of the frequencies and divide each frequency by that sum to get probabilities\n",
    "        \n",
    "        #unigram\n",
    "        self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
    "        \n",
    "        #bigram\n",
    "        for outer_key,inner_dict in self.bigram.items():\n",
    "            inner_dict_probs = {k:v/sum(inner_dict.values()) for (k,v) in inner_dict.items()}\n",
    "            self.bigram[outer_key] = inner_dict_probs\n",
    "            \n",
    "                   \n",
    "    def get_prob(self,token,context=\"\",method=\"unigram\"):\n",
    "        #simple look up method\n",
    "        if method==\"unigram\":\n",
    "            return self.unigram.get(token,self.unigram[\"_UNK\"])\n",
    "        elif method==\"bigram\":\n",
    "            #return self.bigram.get(context[-1],self.bigram[\"_UNK\"]).get(token,self.bigram[context[-1]][\"_UNK\"])\n",
    "            \n",
    "            bigram=self.bigram.get(context[-1],self.bigram.get(\"_UNK\",{}))\n",
    "            big_p=bigram.get(token,bigram.get(\"_UNK\",0))\n",
    "            lmbda=bigram[\"__DISCOUNT\"]\n",
    "            uni_p=self.unigram.get(token,self.unigram.get(\"_UNK\",0))\n",
    "            #print(big_p,lmbda,uni_p)\n",
    "            p=big_p+lmbda*uni_p            \n",
    "            return p\n",
    "    \n",
    "    def get_common_words(self,k=10,stopword='.'):\n",
    "        \n",
    "        sorted_unigram = sorted(self.unigram.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "        chosen_words = []\n",
    "        for i in range(k):\n",
    "            rand_word = random.choice(sorted_unigram[:k])\n",
    "            chosen_words.append(rand_word)\n",
    "            if rand_word[0] == stopword:\n",
    "                break\n",
    "                \n",
    "        return chosen_words\n",
    "        \n",
    "    def get_sample_from_dist(self,k=10,stopword='__END', method=\"unigram\"):\n",
    "        #return one of the highly probable words\n",
    "        \n",
    "        current_word = \"__START\"\n",
    "        blacklist = [\"__START\"]\n",
    "\n",
    "        if method == \"unigram\":\n",
    "            sorted_unigram = sorted(self.unigram.items(), key=lambda item: item[1], reverse=True)\n",
    "            filtered = [(w,p) for (w,p) in sorted_unigram if w not in blacklist]\n",
    "            values,dist = zip(*filtered[:k])\n",
    "        \n",
    "            chosen_words = []\n",
    "            for i in range(k):\n",
    "                current_word = random.choices(values,dist)[0]\n",
    "                if current_word == stopword:\n",
    "                    break\n",
    "                chosen_words.append(current_word)\n",
    "\n",
    "            return \" \".join(chosen_words[:-1])\n",
    "        \n",
    "        \n",
    "        elif method == \"bigram\":       \n",
    "            sorted_bigram = sorted(self.bigram[current_word].items(), key=lambda item: item[1], reverse=True)\n",
    "            filtered = [(w,p) for (w,p) in sorted_bigram if w not in blacklist]\n",
    "            values,dist = zip(*filtered[:k])\n",
    "            \n",
    "            chosen_words = []\n",
    "            for i in range(k):\n",
    "                sorted_bigram = sorted(self.bigram[current_word].items(), key=lambda item: item[1], reverse=True)\n",
    "                filtered = [(w,p) for (w,p) in sorted_bigram if w not in blacklist]\n",
    "                values,dist = zip(*filtered[:k])\n",
    "                current_word = random.choices(values,dist)[0]\n",
    "                if current_word == stopword:\n",
    "                    break\n",
    "                chosen_words.append(current_word)\n",
    "            \n",
    "            return \" \".join(chosen_words[:-1])\n",
    "        \n",
    "    def compute_prob_line(self,line,method=\"unigram\"):\n",
    "        #this will add _start to the beginning of a line of text\n",
    "        #compute the probability of the line according to the desired model\n",
    "        #and returns probability together with number of tokens\n",
    "    \n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        acc=0\n",
    "        for i,token in enumerate(tokens[1:]):\n",
    "            acc+=math.log(self.get_prob(token,tokens[:i+1],method))\n",
    "        return acc,len(tokens[1:])\n",
    "       \n",
    "    \n",
    "    def compute_probability(self,filenames=[],method=\"unigram\"):\n",
    "        #computes the probability (and length) of a corpus contained in filenames\n",
    "        if filenames==[]:\n",
    "            filenames=self.files\n",
    "        \n",
    "        total_p=0\n",
    "        total_N=0\n",
    "        for i,afile in enumerate(filenames):\n",
    "            print(\"Processing file {}:{}\".format(i,afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            p,N=self.compute_prob_line(line,method=method)\n",
    "                            total_p+=p\n",
    "                            total_N+=N\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing file {}: ignoring rest of file\".format(afile))\n",
    "        return total_p,total_N\n",
    "    \n",
    "    def compute_perplexity(self,filenames=[],method=\"unigram\"):\n",
    "        #compute the probability and length of the corpus\n",
    "        #calculate perplexity\n",
    "        #lower perplexity means that the model better explains the data\n",
    "        \n",
    "        p,N=self.compute_probability(filenames=filenames,method=method)\n",
    "        #print(p,N)\n",
    "        pp=math.exp(-p/N)\n",
    "        return pp   \n",
    "    \n",
    "    \n",
    "    def apply_discount(self, discount=0.75):\n",
    "        \"\"\" for outer,inner in self.bigram.items():\n",
    "            total_discount = 0\n",
    "            for w,c in inner.items():\n",
    "                self.bigram[outer][w] -= 0.75\n",
    "                total_discount += 0.75\n",
    "            \n",
    "            self.bigram[outer][\"_DISCOUNT\"] = total_discount\"\"\"\n",
    "            \n",
    "            \n",
    "        self.bigram={k:{kk:value-discount for (kk,value) in adict.items()}for (k,adict) in self.bigram.items()}\n",
    "        \n",
    "        #for each word, store the total amount of the discount so that the total is the same \n",
    "        #i.e., so we are reserving this as probability mass\n",
    "        for k in self.bigram.keys():\n",
    "            lamb=len(self.bigram[k])\n",
    "            self.bigram[k][\"__DISCOUNT\"]=lamb*discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DYNMT10.TXT\n",
      "Processing 09TOM10.TXT\n",
      "Processing PRSIT10.TXT\n",
      "Processing NWIND10.TXT\n",
      "Processing BDAPH10.TXT\n"
     ]
    }
   ],
   "source": [
    "model_2=language_model(files=trainingfiles[:MAX_FILES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in': 0.05508474576271186,\n",
       " '.': 0.1228813559322034,\n",
       " ',': 0.17372881355932204,\n",
       " 'and': 0.05508474576271186,\n",
       " 'grew': 0.038135593220338986,\n",
       " '_UNK': 0.4788135593220339,\n",
       " '_DISCOUNT': 0.07627118644067797}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.bigram[\"grass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DYNMT10.TXT\n",
      "Processing 09TOM10.TXT\n",
      "Processing PRSIT10.TXT\n",
      "Processing NWIND10.TXT\n",
      "Processing BDAPH10.TXT\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "dictionary changed size during iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m MAX_FILES\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[0;32m      3\u001b[0m filesets\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m:trainingfiles[:MAX_FILES],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m\"\u001b[39m:heldoutfiles[:MAX_FILES]}\n\u001b[1;32m----> 6\u001b[0m my_lm_2\u001b[38;5;241m=\u001b[39m\u001b[43mlanguage_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m methods\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munigram\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigram\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#methods=[\"bigram\"]\u001b[39;00m\n",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36mlanguage_model.__init__\u001b[1;34m(self, trainingdir, files)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_dir\u001b[38;5;241m=\u001b[39mtrainingdir\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles\u001b[38;5;241m=\u001b[39mfiles\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36mlanguage_model.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munigram\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#consider rare words as unknown\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_unknown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#apply discount to the bigrams\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_discount()\n",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36mlanguage_model.create_unknown\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m unk_inner_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbigram[outer][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_UNK\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w,f \u001b[38;5;129;01min\u001b[39;00m inner\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbigram[outer][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_UNK\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbigram[outer]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_UNK\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m f\n",
      "\u001b[1;31mRuntimeError\u001b[0m: dictionary changed size during iteration"
     ]
    }
   ],
   "source": [
    "MAX_FILES=5\n",
    "\n",
    "filesets={\"training\":trainingfiles[:MAX_FILES],\"testing\":heldoutfiles[:MAX_FILES]}\n",
    "\n",
    "\n",
    "my_lm_2=language_model(files=filesets[\"training\"])\n",
    "methods=[\"unigram\",\"bigram\"]\n",
    "#methods=[\"bigram\"]\n",
    "\n",
    "for f,names in list(filesets.items()):\n",
    "    for m in methods:\n",
    "\n",
    "        p=my_lm.compute_perplexity(filenames=names,method=m)\n",
    "        \n",
    "        print(\"Perplexity on {} with {} method is {}\".format(f,m,p))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
