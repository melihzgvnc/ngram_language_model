{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modelling Lab (week 2)\n",
    "This notebook provides the \"starter\" code in the week 2 lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "We need to get the names of files in the training directory and split them into training and testing 50:50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 522 files in the training directory: sentence-completion/Holmes_Training_Data\n"
     ]
    }
   ],
   "source": [
    "import os,random,math\n",
    "TRAINING_DIR=\"sentence-completion/Holmes_Training_Data\"  #this needs to be the parent directory for the training corpus\n",
    "\n",
    "def get_training_testing(training_dir=TRAINING_DIR,split=0.5):\n",
    "    \n",
    "    filenames=os.listdir(training_dir)\n",
    "    n=len(filenames)\n",
    "    print(\"There are {} files in the training directory: {}\".format(n,training_dir))\n",
    "    random.seed(53)  #if you want the same random split every time\n",
    "    random.shuffle(filenames)\n",
    "    index=int(n*split)\n",
    "    return(filenames[:index],filenames[index:])\n",
    "    \n",
    "trainingfiles,heldoutfiles=get_training_testing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainingfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1  Building a unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize as tokenize\n",
    "import operator\n",
    "import random\n",
    "\n",
    "class language_model():\n",
    "    \n",
    "    def __init__(self,trainingdir=TRAINING_DIR,files=[]):\n",
    "        #store the names of the files containing training data and run the training method\n",
    "        self.training_dir=trainingdir\n",
    "        self.files=files\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "    def train(self):\n",
    "        #initialise an empty dictionary which will be the unigram model {w:P(w)} when training is complete\n",
    "        self.unigram={}\n",
    "        #process all of the training data, accumulating counts of events\n",
    "        self._processfiles()\n",
    "        #convert the accumulated counts to probabilities\n",
    "        self._convert_to_probs()\n",
    "        \n",
    "    def _processline(self,line):\n",
    "        #process each line of a file\n",
    "        #each line is tokenized and has a special start and end token added\n",
    "        #counts of tokens are added to the self.unigram count model\n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        for token in tokens:\n",
    "            self.unigram[token]=self.unigram.get(token,0)+1\n",
    "    \n",
    "    \n",
    "    def _processfiles(self):\n",
    "        #process each file in turn\n",
    "        for afile in self.files:\n",
    "            print(\"Processing {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            self._processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n",
    "      \n",
    "            \n",
    "    def _convert_to_probs(self):\n",
    "        #self.unigram initially counts counts for each token {token:freq(token)}\n",
    "        #sum all of the frequencies and divide each frequency by that sum to get probabilities\n",
    "        \n",
    "        self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
    "    \n",
    "    \n",
    "    def get_prob(self,token,method=\"unigram\"):\n",
    "        #simple look up method\n",
    "        if method==\"unigram\":\n",
    "            return self.unigram.get(token,0)\n",
    "        else:\n",
    "            print(\"Not implemented: {}\".format(method))\n",
    "            return 0  \n",
    "    \n",
    "    def get_common_words(self,k=10,stopword=\"__END\"):\n",
    "        #keep returning one of the highly probable words until a stopword is encountered or the max length is exceeded\n",
    "        \n",
    "        blacklist = [\"__START\"]\n",
    "        sorted_unigram = sorted(self.unigram.items(), key=lambda item: item[1], reverse=True)\n",
    "        filtered = [w for (w,p) in sorted_unigram if w not in blacklist]\n",
    "        \n",
    "        chosen_words = []\n",
    "        for i in range(k):\n",
    "            rand_word = random.choice(filtered[:k])\n",
    "            if rand_word == stopword:\n",
    "                break\n",
    "            chosen_words.append(rand_word)\n",
    "                \n",
    "        return \" \".join(chosen_words[:-1])\n",
    "        \n",
    "        \n",
    "    def get_sample_from_dist(self,k=10,stopword='__END'):\n",
    "        #return one of the highly probable words\n",
    "        \n",
    "        blacklist = [\"__START\"]\n",
    "        sorted_unigram = sorted(self.unigram.items(), key=lambda item: item[1], reverse=True)\n",
    "        filtered = [(w,p) for (w,p) in sorted_unigram if w not in blacklist]\n",
    "        values,dist = zip(*filtered[:k])\n",
    "        \n",
    "        chosen_words = []\n",
    "        for i in range(k):\n",
    "            rand_word = random.choices(values,dist)[0]\n",
    "            if rand_word == stopword:\n",
    "                break\n",
    "            chosen_words.append(rand_word)\n",
    "                \n",
    "        return \" \".join(chosen_words[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DYNMT10.TXT\n",
      "Processing 09TOM10.TXT\n",
      "Processing PRSIT10.TXT\n",
      "Processing NWIND10.TXT\n",
      "Processing BDAPH10.TXT\n"
     ]
    }
   ],
   "source": [
    "MAX_FILES=5\n",
    "mylm=language_model(files=trainingfiles[:MAX_FILES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03527550407241448"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.get_prob('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to the of `` and I , of of'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.get_common_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you look up some probabilities of words in your model.  Pick some words which you would expect to have high probabilities and some words which you would expect to have low probabilities.\n",
    "\n",
    "As an extension, see how these change if you use a bigger portion of the training data to train your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The to Diamond , his had by , it all out '' her I n't all and Diamond , from and . '' . in , the\""
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.get_sample_from_dist(k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize as tokenize\n",
    "import operator\n",
    "import random\n",
    "import math\n",
    "\n",
    "class language_model():\n",
    "    \n",
    "    def __init__(self,trainingdir=TRAINING_DIR,files=[]):\n",
    "        #store the names of the files containing training data and run the training method\n",
    "        self.training_dir=trainingdir\n",
    "        self.files=files\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "    def train(self):\n",
    "        #initialise an empty dictionary which will be the unigram model {w:P(w)} when training is complete\n",
    "        self.unigram={}\n",
    "        #initialise an empty dictionary which will be the bigram model\n",
    "        self.bigram={}\n",
    "        #process all of the training data, accumulating counts of events\n",
    "        self._processfiles()\n",
    "        #words in the corpus\n",
    "        self.word_count = sum(self.unigram.values())\n",
    "        #convert the accumulated counts to probabilities\n",
    "        self._convert_to_probs()\n",
    "        \n",
    "    def _processline(self,line):\n",
    "        #process each line of a file\n",
    "        #each line is tokenized and has a special start and end token added\n",
    "        #counts of tokens are added to the self.unigram count model\n",
    "        \n",
    "        #unigram\n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        for token in tokens:\n",
    "            self.unigram[token]=self.unigram.get(token,0)+1\n",
    "        \n",
    "        #bigram\n",
    "        tokens_bg = [\"__START\"]+tokenize(line)\n",
    "        for index in range(len(tokens_bg)-1):\n",
    "            if self.bigram.get(tokens_bg[index],0) == 0:\n",
    "                self.bigram[tokens_bg[index]] = {}\n",
    "            \n",
    "            self.bigram[tokens_bg[index]][tokens_bg[index+1]] = self.bigram[tokens_bg[index]].get(tokens_bg[index+1],0)+1            \n",
    "    \n",
    "    def _processfiles(self):\n",
    "        #process each file in turn\n",
    "        for afile in self.files:\n",
    "            print(\"Processing {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            self._processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n",
    "      \n",
    "            \n",
    "    def _convert_to_probs(self):\n",
    "        #self.unigram initially counts counts for each token {token:freq(token)}\n",
    "        #sum all of the frequencies and divide each frequency by that sum to get probabilities\n",
    "        \n",
    "        #unigram\n",
    "        self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
    "        \n",
    "        #bigram\n",
    "        for outer_key,inner_dict in self.bigram.items():\n",
    "            inner_dict_probs = {k:v/sum(inner_dict.values()) for (k,v) in inner_dict.items()}\n",
    "            self.bigram[outer_key] = inner_dict_probs\n",
    "            \n",
    "                   \n",
    "    def get_prob(self,token,method=\"unigram\"):\n",
    "        #simple look up method\n",
    "        if method==\"unigram\":\n",
    "            return self.unigram.get(token,0)\n",
    "        else:\n",
    "            print(\"Not implemented: {}\".format(method))\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def get_common_words(self,k=10,stopword='.'):\n",
    "        \n",
    "        sorted_unigram = sorted(self.unigram.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "        chosen_words = []\n",
    "        for i in range(k):\n",
    "            rand_word = random.choice(sorted_unigram[:k])\n",
    "            chosen_words.append(rand_word)\n",
    "            if rand_word[0] == stopword:\n",
    "                break\n",
    "                \n",
    "        return chosen_words\n",
    "        \n",
    "    def get_sample_from_dist(self,k=10,stopword='__END', method=\"unigram\"):\n",
    "        #return one of the highly probable words\n",
    "        \n",
    "        current_word = \"__START\"\n",
    "        blacklist = [\"__START\"]\n",
    "\n",
    "        if method == \"unigram\":\n",
    "            sorted_unigram = sorted(self.unigram.items(), key=lambda item: item[1], reverse=True)\n",
    "            filtered = [(w,p) for (w,p) in sorted_unigram if w not in blacklist]\n",
    "            values,dist = zip(*filtered[:k])\n",
    "        \n",
    "            chosen_words = []\n",
    "            for i in range(k):\n",
    "                current_word = random.choices(values,dist)[0]\n",
    "                if current_word == stopword:\n",
    "                    break\n",
    "                chosen_words.append(current_word)\n",
    "\n",
    "            return \" \".join(chosen_words[:-1])\n",
    "        \n",
    "        \n",
    "        elif method == \"bigram\":       \n",
    "            sorted_bigram = sorted(self.bigram[current_word].items(), key=lambda item: item[1], reverse=True)\n",
    "            filtered = [(w,p) for (w,p) in sorted_bigram if w not in blacklist]\n",
    "            values,dist = zip(*filtered[:k])\n",
    "            \n",
    "            chosen_words = []\n",
    "            for i in range(k):\n",
    "                sorted_bigram = sorted(self.bigram[current_word].items(), key=lambda item: item[1], reverse=True)\n",
    "                filtered = [(w,p) for (w,p) in sorted_bigram if w not in blacklist]\n",
    "                values,dist = zip(*filtered[:k])\n",
    "                current_word = random.choices(values,dist)[0]\n",
    "                if current_word == stopword:\n",
    "                    break\n",
    "                chosen_words.append(current_word)\n",
    "            \n",
    "            return \" \".join(chosen_words[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DYNMT10.TXT\n",
      "Processing 09TOM10.TXT\n",
      "Processing PRSIT10.TXT\n",
      "Processing NWIND10.TXT\n",
      "Processing BDAPH10.TXT\n"
     ]
    }
   ],
   "source": [
    "lang_ml = language_model(files=trainingfiles[:MAX_FILES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"`` I know what a new copy of you ? '' replied Zero . `` That was . And if he had seen her a few words to the room , he was one that , to the door\""
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_ml.get_sample_from_dist(method=\"bigram\", k=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize as tokenize\n",
    "import operator\n",
    "import random\n",
    "import math\n",
    "\n",
    "class language_model():\n",
    "    \n",
    "    def __init__(self,trainingdir=TRAINING_DIR,files=[]):\n",
    "        #store the names of the files containing training data and run the training method\n",
    "        self.training_dir=trainingdir\n",
    "        self.files=files\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "    def train(self):\n",
    "        #initialise an empty dictionary which will be the unigram model {w:P(w)} when training is complete\n",
    "        self.unigram={}\n",
    "        #initialise an empty dictionary which will be the bigram model\n",
    "        self.bigram={}\n",
    "        #process all of the training data, accumulating counts of events\n",
    "        self._processfiles()\n",
    "        #words in the corpus\n",
    "        self.word_count = sum(self.unigram.values())\n",
    "        #convert the accumulated counts to probabilities\n",
    "        self._convert_to_probs()\n",
    "        \n",
    "    def _processline(self,line):\n",
    "        #process each line of a file\n",
    "        #each line is tokenized and has a special start and end token added\n",
    "        #counts of tokens are added to the self.unigram count model\n",
    "        \n",
    "        #unigram\n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        for token in tokens:\n",
    "            self.unigram[token]=self.unigram.get(token,0)+1\n",
    "        \n",
    "        #bigram\n",
    "        tokens_bg = [\"__START\"]+tokenize(line)\n",
    "        for index in range(len(tokens_bg)-1):\n",
    "            if self.bigram.get(tokens_bg[index],0) == 0:\n",
    "                self.bigram[tokens_bg[index]] = {}\n",
    "            \n",
    "            self.bigram[tokens_bg[index]][tokens_bg[index+1]] = self.bigram[tokens_bg[index]].get(tokens_bg[index+1],0)+1            \n",
    "    \n",
    "    def _processfiles(self):\n",
    "        #process each file in turn\n",
    "        for afile in self.files:\n",
    "            print(\"Processing {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            self._processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n",
    "      \n",
    "            \n",
    "    def _convert_to_probs(self):\n",
    "        #self.unigram initially counts counts for each token {token:freq(token)}\n",
    "        #sum all of the frequencies and divide each frequency by that sum to get probabilities\n",
    "        \n",
    "        #unigram\n",
    "        self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
    "        \n",
    "        #bigram\n",
    "        for outer_key,inner_dict in self.bigram.items():\n",
    "            inner_dict_probs = {k:v/sum(inner_dict.values()) for (k,v) in inner_dict.items()}\n",
    "            self.bigram[outer_key] = inner_dict_probs\n",
    "            \n",
    "                   \n",
    "    def get_prob(self,token,method=\"unigram\"):\n",
    "        #simple look up method\n",
    "        if method==\"unigram\":\n",
    "            return self.unigram.get(token,0)\n",
    "        else:\n",
    "            print(\"Not implemented: {}\".format(method))\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def get_common_words(self,k=10,stopword='.'):\n",
    "        \n",
    "        sorted_unigram = sorted(self.unigram.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "        chosen_words = []\n",
    "        for i in range(k):\n",
    "            rand_word = random.choice(sorted_unigram[:k])\n",
    "            chosen_words.append(rand_word)\n",
    "            if rand_word[0] == stopword:\n",
    "                break\n",
    "                \n",
    "        return chosen_words\n",
    "        \n",
    "    def get_sample_from_dist(self,k=10,stopword='__END', method=\"unigram\"):\n",
    "        #return one of the highly probable words\n",
    "        \n",
    "        current_word = \"__START\"\n",
    "        blacklist = [\"__START\"]\n",
    "\n",
    "        if method == \"unigram\":\n",
    "            sorted_unigram = sorted(self.unigram.items(), key=lambda item: item[1], reverse=True)\n",
    "            filtered = [(w,p) for (w,p) in sorted_unigram if w not in blacklist]\n",
    "            values,dist = zip(*filtered[:k])\n",
    "        \n",
    "            chosen_words = []\n",
    "            for i in range(k):\n",
    "                current_word = random.choices(values,dist)[0]\n",
    "                if current_word == stopword:\n",
    "                    break\n",
    "                chosen_words.append(current_word)\n",
    "\n",
    "            return \" \".join(chosen_words[:-1])\n",
    "        \n",
    "        \n",
    "        elif method == \"bigram\":       \n",
    "            sorted_bigram = sorted(self.bigram[current_word].items(), key=lambda item: item[1], reverse=True)\n",
    "            filtered = [(w,p) for (w,p) in sorted_bigram if w not in blacklist]\n",
    "            values,dist = zip(*filtered[:k])\n",
    "            \n",
    "            chosen_words = []\n",
    "            for i in range(k):\n",
    "                sorted_bigram = sorted(self.bigram[current_word].items(), key=lambda item: item[1], reverse=True)\n",
    "                filtered = [(w,p) for (w,p) in sorted_bigram if w not in blacklist]\n",
    "                values,dist = zip(*filtered[:k])\n",
    "                current_word = random.choices(values,dist)[0]\n",
    "                if current_word == stopword:\n",
    "                    break\n",
    "                chosen_words.append(current_word)\n",
    "            \n",
    "            return \" \".join(chosen_words[:-1])\n",
    "    def compute_prob_line(self,line,method=\"unigram\"):\n",
    "        #this will add _start to the beginning of a line of text\n",
    "        #compute the probability of the line according to the desired model\n",
    "        #and returns probability together with number of tokens\n",
    "    \n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        acc=0\n",
    "        for i,token in enumerate(tokens[1:]):\n",
    "            acc+=math.log(self.get_prob(token,tokens[:i+1],method))\n",
    "        return acc,len(tokens[1:])\n",
    "       \n",
    "    \n",
    "    def compute_probability(self,filenames=[],method=\"unigram\"):\n",
    "        #computes the probability (and length) of a corpus contained in filenames\n",
    "        if filenames==[]:\n",
    "            filenames=self.files\n",
    "        \n",
    "        total_p=0\n",
    "        total_N=0\n",
    "        for i,afile in enumerate(filenames):\n",
    "            print(\"Processing file {}:{}\".format(i,afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            p,N=self.compute_prob_line(line,method=method)\n",
    "                            total_p+=p\n",
    "                            total_N+=N\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing file {}: ignoring rest of file\".format(afile))\n",
    "        return total_p,total_N\n",
    "    \n",
    "    def compute_perplexity(self,filenames=[],method=\"unigram\"):\n",
    "        \n",
    "        #compute the probability and length of the corpus\n",
    "        #calculate perplexity\n",
    "        #lower perplexity means that the model better explains the data\n",
    "        \n",
    "        p,N=self.compute_probability(filenames=filenames,method=method)\n",
    "        #print(p,N)\n",
    "        pp=math.exp(-p/N)\n",
    "        return pp   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized empty Git repository in C:/Users/melih/Desktop/MSc AI & Adaptive Systems/Term 2/Advanced NLP/Labs/week 2/.git/\n"
     ]
    }
   ],
   "source": [
    "!git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
